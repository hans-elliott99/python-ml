{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep some word data (Shakespeare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115393\n"
     ]
    }
   ],
   "source": [
    "# read it in to inspect it\n",
    "with open('../makemore_data/tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hello world\"))\n",
    "print(decode(encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115393]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58])\n"
     ]
    }
   ],
   "source": [
    "# convert data to tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:5]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "# Block-size\n",
    "block_size = 8 ##maximum sequence length (though we add +1)\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([1, 8])\n",
      "targets:\n",
      "torch.Size([1, 8])\n",
      "----\n",
      "when input is [53] the target: 59\n",
      "when input is [53, 59] the target: 6\n",
      "when input is [53, 59, 6] the target: 1\n",
      "when input is [53, 59, 6, 1] the target: 58\n",
      "when input is [53, 59, 6, 1, 58] the target: 56\n",
      "when input is [53, 59, 6, 1, 58, 56] the target: 47\n",
      "when input is [53, 59, 6, 1, 58, 56, 47] the target: 40\n",
      "when input is [53, 59, 6, 1, 58, 56, 47, 40] the target: 59\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 1 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split, block_size=8):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention\n",
    "\n",
    "Based on Attention is All you Need: https://arxiv.org/abs/1706.03762\n",
    "\n",
    "\n",
    "## the mathematical trick (weighted aggregation by matrix multiplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 #batch, time, channels/features\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want tokens to share context with previous time-steps\n",
    "# A simple/weak way would be to average the information from the preceding time-steps\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] #select prev-timesteps, and the current one. out: (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0) #average C across time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can do this more efficiently using matrix multiplication\n",
    "wei = torch.tril(torch.ones(T, T)) #weights of weighted aggregation\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)\n",
    "xbow2 = wei @ x #(T, T) @ (B, T, C) ---> (B, T, C)\n",
    "\n",
    "# Same result as above, via MatMul (much faster!)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "a @ b = c:\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# Why does this work???\n",
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "# adding tril, for a triangle matrix,allows us to zero out \"future\" elements,\n",
    "# essentially filtering out the future time-steps\n",
    "a = torch.tril(torch.ones(3, 3)) \n",
    "# Modifying this initial matrix so that each row is 1/row_number weights the sums,\n",
    "# so that they sum to 1, which means c = a @ b gives us an average\n",
    "# (torch.sum, dim=1 keepdims=True, gives us (1+0+0 = 1), (1+1+0 = 2), (1+1+1 = 3))\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('a @ b = c:')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "# We can also modify this process for the same result\n",
    "# masked_fill : for all elements where tril == 0 become -inf\n",
    "# we then softmax each row, which normalizes - 0s become one, -inf becomes 0, and we sum and divide\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = torch.nn.functional.softmax(wei, dim=1)\n",
    "\n",
    "print(wei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we learned the weights of wei (or 'a' above) instead of setting them to be the mean??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention (Single Head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 4: self-attention! (for a singular head)\n",
    "\n",
    "# We don't want our 'wei' matrix to be uniform/pre-determined, we want it to be data-dependent!\n",
    "# Every single token at each position will emit 3 vectors: a query, a key, and a value\n",
    "#   query: contains info on what the token is looking for\n",
    "#   key  : contains info on what the token contains\n",
    "#   value: x is the 'private' token information, and value determines what to share/communicate when looking at the keys of other tokens \n",
    "# We get 'affinities'/attention scores between tokens in the sequence via a dot product between the \n",
    "# token's query and the keys of all the other tokens!\n",
    "# If the key and query are well-aligned, they will interact more, and wei will have a higher value at\n",
    "# their combined positions, so when we softmax, we will get much more of the info from that token's features\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# a single Head\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "# retrieve key and query\n",
    "k = key(x)    # B, T, head_size\n",
    "q = query(x)  # B, T, head_size\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) #transpose last 2 dims, so (B, T, 16) @ (B, 16, T) = (B, T, T)\n",
    "\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) #still want to prevent \"seeing the future\"\n",
    "wei = torch.nn.functional.softmax(wei, dim=1)   #aggregate so weights sum to 1\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an **\"encoder\"** attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a **\"decoder\"** attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k variance = 0.9873070120811462 | q variance = 1.009503960609436\n",
      "wei variance pre-scale = 15.663772583007812, post-scale = 0.9789857864379883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why scale attention? (by 1 / sqrt(head_size))\n",
    "# Unit gaussian inputs\n",
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "# Wei\n",
    "wei_ = q @ k.transpose(-2, -1) \n",
    "# Wei scaled\n",
    "wei = wei_ * head_size**-0.5\n",
    "\n",
    "print(f\"k variance = {k.var()} | q variance = {q.var()}\")\n",
    "print(f\"wei variance pre-scale = {wei_.var()}, post-scale = {wei.var()}\")\n",
    "\n",
    "# if Softmax receives rather large values, it will converge towards the largest number\n",
    "# ie, Softmax is too \"peaky\" if we don't control the variance\n",
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "Simply applying multiple self-attention heads in parallel, and concatenating the results.\n",
    "Why?\n",
    "It helps to have multiple channels of communication for tokens to odentify alike/useful tokens."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeedForward Layers + Blocks\n",
    "Once the tokens have had time to look at each-other, we want to add layers which allow computation on the features created by the attention process.  \n",
    "In other words, we give the network some time to make sense of the information it's extracted.  \n",
    "\n",
    "Transformer networks intersperse attention layers (\"communication\") with feed-forward (\"computation\") layers.  \n",
    "\n",
    "But, as the network gets deeper, we need to help counteract optimization issues via:\n",
    "1. skip/residual connections\n",
    "    (We transform the data, but then add the original features back to the transformed data)  \n",
    "    In practice, this means we have a \"residual pathway\" which we can fork off via transformations, but which then come back to the pathway via addition.    \n",
    "    Then, in backwards pass, addition forks the gradient equally.  \n",
    "2. In the MultiHeadAttention layers and FeedForwards layers, we want add a \"projection\" layer to project the attention's output back into the residual pathway.  \n",
    " \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm\n",
    "Similair to BatchNorm, except that instead of normalizing across the batch, we normalize across the sample (so we don't need any running buffers, no distinction between training and test time...)  \n",
    "\"Normalizes the rows instead of the columns\"  \n",
    "\n",
    "In departure from Attention is All You Need, it is more common now to apply layer norm before the feed-forward step... the \"pre-norm formulation\"  \n",
    "In our case, it is essentially normalizing the 'per-token' features, treating both the batch and time dimensions as batch dimensions.  \n",
    "\n",
    "We add LayerNorm to our Transformer Blocks and we add one final layer norm after all the blocks and before the final linear layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttHead(nn.Module):\n",
    "    def __init__(self, block_size, head_size, in_feats, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(in_feats, head_size, bias=False)\n",
    "        self.query = nn.Linear(in_feats, head_size, bias=False)\n",
    "        self.value = nn.Linear(in_feats, head_size, bias=False)\n",
    "        # buffer: since tril is not a module, assign using register buffer\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # Compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 #scaled, out:(B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #(B,T,T)\n",
    "        wei = F.softmax(wei, dim=1)\n",
    "        wei = self.dropout(wei)\n",
    "        # weigted aggregation\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, block_size, num_heads, head_size, in_feats, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttHead(block_size, head_size, in_feats, dropout) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.proj = nn.Linear(in_feats, in_feats)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Concatenate over channel/feature dimension (last dim)\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return self.dropout(out)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, in_feats, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_feats, 4*in_feats), #times 4, as in Attention is All You Need\n",
    "            nn.ReLU(),\n",
    "            # project back into residual pathway\n",
    "            nn.Linear(4*in_feats, in_feats),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block\n",
    "    LayerNorm\n",
    "    MultiHeadAttention\n",
    "    LayerNorm\n",
    "    FeedForward\n",
    "    \"\"\"\n",
    "    def __init__(self, block_size, in_feats, n_heads, dropout) -> None:\n",
    "        super().__init__()\n",
    "        head_size = in_feats // n_heads\n",
    "        self.ln1 = nn.LayerNorm(in_feats)\n",
    "        self.sa = MultiHeadAttention(block_size, n_heads, head_size, in_feats, dropout)\n",
    "        self.ln2 = nn.LayerNorm(in_feats)\n",
    "        self.ffwd = FeedForward(in_feats, dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) #x+... = skip connections\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, block_size, n_embed, \n",
    "                       n_layers, heads_per_layer,\n",
    "                       dropout):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(block_size, n_embed, heads_per_layer, dropout) for _ in range(n_layers)]\n",
    "            )\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,n_embed)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T)) # (T, n_embed)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, block_size, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to be last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed    = 32      #\n",
    "block_size = 256\n",
    "n_layers   = 4\n",
    "heads_per_layer = 4\n",
    "dropout = 0.\n",
    "\n",
    "m = TransformerModel(vocab_size=vocab_size,\n",
    "                     block_size=block_size,\n",
    "                     n_embed=n_embed,\n",
    "                     n_layers=n_layers,\n",
    "                     heads_per_layer=heads_per_layer,\n",
    "                     dropout=dropout\n",
    "                     )\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 2.868577003479004\n",
      "val loss 2.948580026626587\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "m.train()\n",
    "for steps in range(1): # increase number of steps for good results... \n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', block_size)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"train loss\", loss.item())\n",
    "\n",
    "m.eval()\n",
    "xv, yv = get_batch('valid', block_size)\n",
    "logits, vloss = m(xv, yv)\n",
    "print(\"val loss\", vloss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neeoine e e oao,-r?  p v3vrjumemrobotXwtallaOJathE lisylheO\n",
      "Je rmct mlralrie ,VofohanonhooQon eo\n",
      "As moI ey,\n",
      "& !Q iroIXwerendnoee nd lgeu,py'cg T3Ysork esgseromi t' s at thfs mmled Tf bsd rthmn g pelmn foms\n",
      "T\n",
      "Tge s Jl tke er:\n",
      "Lhowt'sr loureusne e, e t ?\n",
      "ElelBd $lrayaRkl'uafn$iG tkofwYof e homnRlrohin n hOhIKin,Oh\n",
      "Ue d tenB :\n",
      "-leaset Sbin,r ore ?CGcI\n",
      "TSeeSdhesKoheac, iy fwviwcOncthos re ': mtVd I sI,t. l y;\n",
      "cy wa acore, d o athnng, eesreri r weriy:\n",
      "\n",
      "jy .\n",
      "Wir h cisy sthf I:\n",
      "ohw,ohs s,l ssh, io 'alf\n"
     ]
    }
   ],
   "source": [
    "strt_tok = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(\n",
    "    decode(m.generate(idx=strt_tok, block_size=block_size, max_new_tokens=500)[0].tolist())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time 1:42:32 vid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ca1aac19293434683a90a38e84457b4a988d546cab5af6e54fadfac586c8431"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
